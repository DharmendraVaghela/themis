import json

import pandas

from themis import ANSWER, ANSWER_ID, TITLE, FILENAME, QUESTION, logger, CONFIDENCE, CsvFileType, IN_PURVIEW, CORRECT, \
    pretty_print_json

QUESTION_TEXT = "Question_Text"
IS_IN_PURVIEW = "Is_In_Purview"
SYSTEM_ANSWER = "System_Answer"
ANNOTATION_SCORE = "Annotation_Score"
TOP_ANSWER_TEXT = "TopAnswerText"
TOP_ANSWER_CONFIDENCE = "TopAnswerConfidence"
ANS_LONG = "ANS_LONG"
ANS_SHORT = "ANS_SHORT"
IS_ON_TOPIC = "IS_ON_TOPIC"


def annotation_assist_qa_input(answers, questions, judgments):
    """
    Create list of Q&A pairs for judgment by Annotation Assist.

    The Q&A pairs to be judged are compiled from sets of answers generated by Q&A systems. These may be filtered by an
    optional list of questions. Judgements may be taken from optional sets of previously judged Q&A pairs.

    :param answers: answers to questions as generated by Q&A systems
    :type answers: pandas.DataFrame
    :param questions: optional set of questions to filter on, if None use all answered questions
    :type questions: pandas.DataFrame
    :param judgments: optional judgments, look up a judgment here before sending the Q&A pair to Annotation Assist
    :type judgments: pandas.DataFrame
    :return: Q&A pairs to pass to Annotation Assist for judgment
    :rtype: pandas.DataFrame
    """
    qa_pairs = pandas.concat(answers)
    qa_pairs = qa_pairs.drop_duplicates([QUESTION, ANSWER])
    logger.info("%d Q&A pairs" % len(qa_pairs))
    if questions is not None:
        qa_pairs = qa_pairs[qa_pairs[QUESTION].isin(questions)]
        logger.info("%d Q&A pairs for %d unique questions" % (len(qa_pairs), len(questions)))
    if judgments:
        judged_qa_pairs = pandas.concat(judgments)
        # There should be no duplicates.
        judged_qa_pairs = judged_qa_pairs.drop_duplicates([QUESTION, ANSWER])
        judged = pandas.merge(qa_pairs, judged_qa_pairs, on=(QUESTION, ANSWER))
        not_judged = qa_pairs[~qa_pairs[[QUESTION, ANSWER]].isin(judged[[QUESTION, ANSWER]])]
        logger.info("%d unjudged Q&A pairs" % len(not_judged))
    else:
        not_judged = qa_pairs
    not_judged = not_judged.rename(
        columns={QUESTION: QUESTION_TEXT, ANSWER: TOP_ANSWER_TEXT, CONFIDENCE: TOP_ANSWER_CONFIDENCE})
    not_judged = not_judged[[QUESTION_TEXT, TOP_ANSWER_TEXT, TOP_ANSWER_CONFIDENCE]]
    return not_judged


def create_annotation_assist_corpus(corpus):
    corpus["splitPauTitle"] = corpus[TITLE].apply(lambda title: title.split(":"))
    corpus = corpus.rename(columns={ANSWER: "text", ANSWER_ID: "pauId", TITLE: "title", FILENAME: "fileName"})
    return pretty_print_json(json.loads(corpus.to_json(orient="records"), encoding="utf-8"))


def mark_annotation_assist_correct(annotation_assist, judgment_threshold):
    """
    Convert the annotation score column to a boolean correct column by applying a threshold.

    :param annotation_assist: Annotation Assist judgments
    :type annotation_assist: pandas.DataFrame
    :param judgment_threshold: threshold above which an answer is deemed correct
    :type judgment_threshold: pandas.DataFrame
    :return: Annotation Assist judgments with a boolean Correct column
    :rtype: pandas.DataFrame
    """
    annotation_assist[CORRECT] = annotation_assist[ANNOTATION_SCORE] >= judgment_threshold
    return annotation_assist.drop(ANNOTATION_SCORE, axis="columns")


class AnnotationAssistFileType(CsvFileType):
    """
    Read the file produced by the `Annotation Assist <https://github.com/cognitive-catalyst/annotation-assist>` tool.
    """

    def __init__(self):
        super(self.__class__, self).__init__([QUESTION_TEXT, IS_IN_PURVIEW, SYSTEM_ANSWER, ANNOTATION_SCORE],
                                             {QUESTION_TEXT: QUESTION, IS_IN_PURVIEW: IN_PURVIEW,
                                              SYSTEM_ANSWER: ANSWER})

    def __call__(self, filename):
        annotation_assist = super(self.__class__, self).__call__(filename)
        annotation_assist[IN_PURVIEW] = annotation_assist[IN_PURVIEW].astype("bool")
        return annotation_assist[[QUESTION, ANSWER, IN_PURVIEW, ANNOTATION_SCORE]]


def add_judgments_and_frequencies_to_qa_pairs(system_answers, judgments, question_frequencies):
    """
    Collate system answer confidences and annotator judgments by question/answer pair.
    Add to each pair the question frequency.

    Though you expect the set of question/answer pairs in the system answers and judgments to not be disjoint, it may
    be the case that neither is a subset of the other. If annotation is incomplete, there may be Q/A pairs in the
    system answers that haven't been annotated yet. If multiple systems are being judged, there may be Q/A pairs in the
    judgements that don't appear in the system answers.

    :param system_answers: question, answer, and confidence provided by a Q&A system
    :type qa_pairs: pandas.DataFrame
    :param judgments: question, answer, in purview, and judgement provided by annotators
    :type judgments: pandas.DataFrame
    :param question_frequencies: question and question frequency in the test set
    :type question_frequencies: pandas.DataFrame
    :return: question and answer pairs with confidence, in purview, judgement and question frequency
    :rtype: pandas.DataFrame
    """
    # The Annotation Assist tool strips newlines, so remove them from the answer text in the system output as well.
    system_answers[ANSWER] = system_answers[ANSWER].str.replace("\n", "")
    system_answers = pandas.merge(system_answers, judgments, on=(QUESTION, ANSWER))
    return pandas.merge(system_answers, question_frequencies, on=QUESTION)
